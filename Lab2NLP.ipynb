{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spelling Check"
      ],
      "metadata": {
        "id": "U44tuUysHRt0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "doSYzMlQhxDB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c19719c5-ca5f-4fd6-9068-fadbccd557ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.2)\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.1\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.25.0-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.25.0 (from python-Levenshtein)\n",
            "  Downloading Levenshtein-0.25.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.1.0 (from Levenshtein==0.25.0->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.25.0 python-Levenshtein-0.25.0 rapidfuzz-3.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install textblob\n",
        "!pip install pyspellchecker\n",
        "!pip install fuzzywuzzy\n",
        "!pip install python-Levenshtein\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VcgNzbR_pCxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f5fba53-9a86-420d-9df3-914be446084a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correction happening\n",
            "Candidate {'japanning', 'happening', 'penning'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "# find those words that may be misspelled\n",
        "misspelled = spell.unknown(['something', 'is', 'hapenning', 'here'])\n",
        "\n",
        "for word in misspelled:\n",
        "    # Get the one `most likely` answer\n",
        "    print(\"Correction\",spell.correction(word))\n",
        "\n",
        "    # Get a list of `likely` options\n",
        "    print(\"Candidate\",spell.candidates(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BfZzymcVpVph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d8e0edd-9e1f-4eb6-af9e-1b572f3d74f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text: hapenning\n",
            "corrected text: happening\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "a = \"hapenning\"\t\t # incorrect spelling\n",
        "print(\"original text: \"+str(a))\n",
        "\n",
        "b = TextBlob(a)\n",
        "\n",
        "# prints the corrected spelling\n",
        "print(\"corrected text: \"+str(b.correct()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create your own Spelling Checker"
      ],
      "metadata": {
        "id": "Q4jY0--FNdBh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ifgEre3Yf4ko"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#List of words in your vocab\n",
        "data=[]\n",
        "with open(\"clean_vocab.txt\",'r',errors=\"ignore\") as input_vocab:\n",
        "  data=[word for word in input_vocab.readlines()]"
      ],
      "metadata": {
        "id": "DoavzDG1NuCK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "67bbf718-6808-4d55-8531-8248368e41b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'clean_vocab.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-28039ac10e2e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#List of words in your vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"clean_vocab.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_vocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'clean_vocab.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wt-T98y-i3u3"
      },
      "outputs": [],
      "source": [
        "# import the fuzzywuzzy module\n",
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEaWeNPJiwac"
      },
      "outputs": [],
      "source": [
        "# change all the words to lowercase\n",
        "data = [i.lower().replace(\"\\n\",\"\") for i in data]\n",
        "\n",
        "# remove all the duplicates in the list\n",
        "data = set(data)\n",
        "\n",
        "# store all the words into a class variable dictionary\n",
        "dictionary = list(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h06U4tsQ2Bc2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# string setter method\n",
        "def check( string_to_check):\n",
        "    # store the string to be checked in a class variable\n",
        "    string_to_check = string_to_check\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqdxpk082Gfm"
      },
      "outputs": [],
      "source": [
        "# this method returns the possible suggestions of the correct words\n",
        "def suggestions(string_to_check):\n",
        "    # store the words of the string to be checked in a list by using a split function\n",
        "    string_words = string_to_check.split()\n",
        "\n",
        "    # a list to store all the possible suggestions\n",
        "    suggestions = []\n",
        "\n",
        "    # loop over the number of words in the string to be checked\n",
        "    for i in range(len(string_words)):\n",
        "\n",
        "        # loop over words in the dictionary\n",
        "        for name in dictionary:\n",
        "\n",
        "            # if the fuzzywuzzy returns the matched value greater than 80\n",
        "            if fuzz.ratio(string_words[i].lower(), name.lower()) >= 75:\n",
        "                # print(\"name\",name)\n",
        "                # print(\"Score\",fuzz.ratio(string_words[i].lower(), name.lower()))\n",
        "                # append the dict word to the suggestion list\n",
        "                suggestions.append(name)\n",
        "\n",
        "    # return the suggestions list\n",
        "    return suggestions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSRKPc4Q2ISM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# this method returns the corrected string of the given input\n",
        "def correct(string_to_check):\n",
        "    # store the words of the string to be checked in a list by using a split function\n",
        "    string_words = string_to_check.split()\n",
        "\n",
        "    # loop over the number of words in the string to be checked\n",
        "    for i in range(len(string_words)):\n",
        "\n",
        "        # initiaze a maximum probability variable to 0\n",
        "        max_percent = 0\n",
        "\n",
        "        # loop over the words in the dictionary\n",
        "        for name in dictionary:\n",
        "\n",
        "            # calulcate the match probability\n",
        "            percent = fuzz.ratio(string_words[i].lower(), name.lower())\n",
        "\n",
        "            # if the fuzzywuzzy returns the matched value greater than 80\n",
        "            if percent >= 75:\n",
        "\n",
        "                # if the matched probability is\n",
        "                if percent > max_percent:\n",
        "\n",
        "                    # change the original value with the corrected matched value\n",
        "                    string_words[i] = name\n",
        "\n",
        "                # change the max percent to the current matched percent\n",
        "                max_percent = percent\n",
        "\n",
        "    # return the cprrected string\n",
        "    return \" \".join(string_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def correct2(string_to_check):\n",
        "  # store the words of the string to be checked in a list by using a split function\n",
        "    string_words = string_to_check.split()\n",
        "    corrected_words=\"\"\n",
        "    # loop over the number of words in the string to be checked\n",
        "    for word in string_words:\n",
        "\n",
        "      correct=process.extractOne(word.lower(),dictionary, scorer=fuzz.token_sort_ratio)[0]\n",
        "\n",
        "      corrected_words+=correct+\" \"\n",
        "    return corrected_words\n",
        "correct2(\"eeh a5brk\")"
      ],
      "metadata": {
        "id": "s2KKPUihHZJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opVtek7e5bbF"
      },
      "outputs": [],
      "source": [
        "\n",
        "string_to_be_checked = \"eeh a5brkk\"\n",
        "check(string_to_be_checked)\n",
        "\n",
        "# print suggestions and correction\n",
        "print (suggestions(string_to_be_checked))\n",
        "print (correct(string_to_be_checked))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VSM"
      ],
      "metadata": {
        "id": "Z-BnwHa3HNVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip 'ACL.zip'"
      ],
      "metadata": {
        "id": "g0YuysNgbqaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv4ngG72aHKK"
      },
      "source": [
        "**Bellow cell imports all the necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkFO0aH-8W5g"
      },
      "source": [
        "import glob\n",
        "import nltk\n",
        "nltk.download('popular');\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import string\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from collections import OrderedDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJd9bSh7aRn5"
      },
      "source": [
        "**give_path: **function** **\n",
        "\n",
        "In below cell, give_path() takes path of the folder containing all the documents.\n",
        "give_path() then read all the documents and store content of each document (in string format) as 'value' of dictionary and it's file name as 'key' in the dictionary.\n",
        "This function the returns the dictionary containing all documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJK8PfVs8q1u"
      },
      "source": [
        "def give_path(fld_path):                             #give path of the folder containing all documents\n",
        "    dic = {}\n",
        "    file_names = glob.glob(fld_path)\n",
        "    files_150 = file_names[0:10]\n",
        "    for file in files_150:\n",
        "        name = file.split('/')[-1]\n",
        "        with open(file, 'r', errors='ignore') as f:\n",
        "            data = f.read()\n",
        "        dic[name] = data\n",
        "    return dic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2f2SMGJaVnO"
      },
      "source": [
        "**wordList_removePuncs: **function** **\n",
        "\n",
        "In the below cell, two functions wordList() and removePuncs() functions have been combined as wordList_removePuncs.\n",
        "This function is passed with dictionary of all documents (returned by give_path()). Function then remove the stop words and punctuations and returns a list of all the words in our collection of documents. This list contains the redundant words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxmwGJ8V9Wco"
      },
      "source": [
        "def wordList_removePuncs(doc_dict):\n",
        "  #  insert code\n",
        "    return wordList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBS9u85AaZb8"
      },
      "source": [
        "**termFrequencyInDoc: **function** **\n",
        "\n",
        "After the above function returns a list of all words in the collection of documents, vocabulary of words is made (I'm making vocabulary in 'main'). This vocabulary and collection of documents in dictionary form (returned by give_path) is passed to below function (termFrequencyInDoc) to find the frequency of words in each documet. This function returns a dictionary in which 'key' is the document name and its 'value' is another dicionary, containing a word as 'key' and its frequency as 'value'. It's actually a dictionary within a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt1vdt949b8M"
      },
      "source": [
        "def termFrequencyInDoc(vocab, doc_dict):\n",
        "    #  insert code\n",
        "    return tf_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mJs2Xr5ad4x"
      },
      "source": [
        "**wordDocFre: **function** **\n",
        "\n",
        "To count the document frequency, vocabulary of words and dictionary of collection of documents if passed to wordDocFre which returns a dictionary containing a word as 'key' and its document frequency as 'value' of dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1HuAAEi9d0t"
      },
      "source": [
        "def wordDocFre(vocab, doc_dict):\n",
        "    #  insert code\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DfO6susakIx"
      },
      "source": [
        "**inverseDocFre: **function** **\n",
        "\n",
        "inverseDocFre calculates the inverse document frequency. It takes vocabulary of words, dictionary of document frequency (returned by wordDocFre) and number of documents in our collection. It returns a dictionary containing inverse DF in which a word is key and its idf score is its value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrgQ8bks9fvW"
      },
      "source": [
        "def inverseDocFre(vocab,doc_fre,length):\n",
        "   #  insert code\n",
        "    return idf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0BEtQfuao7Q"
      },
      "source": [
        "**tfidf: **function** **\n",
        "\n",
        "Function tfidf takes 4 arguments:\n",
        "1. vocabular of words\n",
        "2. term frequencies: which is passed in form of a dictionary\n",
        "3. Inverse DF: passed in form of a dictionary\n",
        "4. Collection of all docs passed in form of a dictionary\n",
        "\n",
        "It returns a dictionary, which is again a dictionary within a dictionary like in case of TF, but it contains score values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unCpXOmt9hnb"
      },
      "source": [
        "def tfidf(vocab,tf,idf_scr,doc_dict):\n",
        "    #  insert code\n",
        "    return tf_idf_scr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwnqluTnaxrw"
      },
      "source": [
        "**vectorSpaceModel: **function** **\n",
        "\n",
        "To find the relevant documents related to query, pass the query to function alonwith collection of documents (dictionary) and tf-idf scores (dictionary returned by tfidf). Function returns the top 5 documents from a collection of about 22k documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69u58ZAI9kDm"
      },
      "source": [
        "def vectorSpaceModel(query, doc_dict,tfidf_scr):\n",
        "    query_vocab = []\n",
        "    #  insert code\n",
        "    return top_5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwI0Or1Ba2N9"
      },
      "source": [
        "**Testing all the functions above and observe the results.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "knsW8y3Pfk-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAYe0y7KGIdW"
      },
      "source": [
        "\n",
        "path = 'ACL/*.txt'\n",
        "docs = give_path(path)                        #returns a dictionary of all docs\n",
        "M = len(docs)                                 #number of files in dataset\n",
        "w_List = wordList_removePuncs(docs)           #returns a list of tokenized words\n",
        "vocab = list(set(w_List))                     #returns a list of unique words\n",
        "tf_dict = termFrequencyInDoc(vocab, docs)     #returns term frequency\n",
        "df_dict = wordDocFre(vocab, docs)             #returns document frequencies\n",
        "idf_dict = inverseDocFre(vocab,df_dict,M)     #returns idf scores\n",
        "tf_idf = tfidf(vocab,tf_dict,idf_dict,docs)   #returns tf-idf socres\n",
        "\n",
        "query1 = 'Text Mining'\n",
        "query2 = 'generative models'\n",
        "query3 = 'topic modeling'\n",
        "query4 = 'Natural language Processing'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)\n",
        "# vocab"
      ],
      "metadata": {
        "id": "YXLp9_pK5Y7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top1 = vectorSpaceModel(query1, docs,tf_idf)    #returns top 5 documents using VSM\n",
        "print('Top 5 Documents for Query 1: \\n', top1)\n"
      ],
      "metadata": {
        "id": "iM9yeuRLhdVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query1_misspelled=\"tevt minng\"\n",
        "q=\"\"\n",
        "for word in query1_misspelled.split():\n",
        "  q+=spell.correction(word)+\" \"\n",
        "\n",
        "print(\"corrected:\",q)\n",
        "\n",
        "top1 = vectorSpaceModel(q, docs,tf_idf)    #returns top 5 documents using VSM\n",
        "print('Top 5 Documents for Query 1: \\n', top1)"
      ],
      "metadata": {
        "id": "serJeV5RclWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "top2 = vectorSpaceModel(query2, docs,tf_idf)    #returns top 5 documents using VSM\n",
        "print('Top 5 Documents for Query 2: \\n', top2)\n"
      ],
      "metadata": {
        "id": "1aVStKUz5DjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top3 = vectorSpaceModel(query3, docs,tf_idf)    #returns top 5 documents using VSM\n",
        "print('Top 5 Documents for Query 3: \\n', top3)\n"
      ],
      "metadata": {
        "id": "Xv6sJsQMhj5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top4 = vectorSpaceModel(query4, docs,tf_idf)    #returns top 5 documents using VSM\n",
        "print('Top 5 Documents for Query 4: \\n', top4)\n"
      ],
      "metadata": {
        "id": "HG6BH9BNhlMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bigram Language Model"
      ],
      "metadata": {
        "id": "4mhFNMGcHI-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load a WhatsApp Chat"
      ],
      "metadata": {
        "id": "fXnrpzUIjGIW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut4wG-HVb2rx"
      },
      "outputs": [],
      "source": [
        "\n",
        "list_of_sentences=[]\n",
        "f = open(\"_chat.txt\",'r',errors=\"ignore\")\n",
        "\n",
        "# with open(\"train_whatsapp.txt\",'r',errors=\"ignore\") as input_txt:\n",
        "for line in f:\n",
        "  # print(line)\n",
        "  if(\"<Speaker 1 Contact Name:>\"in line):\n",
        "    i=line.index(\"<Speaker 1 Contact Name:>\")\n",
        "    text=line[i+len(\"<Speaker 1 Contact Name:>\"):]\n",
        "    list_of_sentences.append(line)\n",
        "  elif(\"<Speaker 2 Contact Name:>\"in line):\n",
        "    i=line.index(\"<Speaker 2 Contact Name:>\")\n",
        "    text=line[i+len(\"<Speaker 2 Contact Name:>\"):]\n",
        "    list_of_sentences.append(text)\n",
        "  else:\n",
        "    list_of_sentences.append(line)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rar3mq2_7hgQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def readData():\n",
        "    data = list_of_sentences[:2000]\n",
        "    dat=[]\n",
        "    for i in range(len(data)):\n",
        "        for word in data[i].split():\n",
        "            dat.append(word.lower())\n",
        "    print(dat)\n",
        "    return dat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryBaNWbb7nH7"
      },
      "outputs": [],
      "source": [
        "\n",
        "def createBigram(data):\n",
        "   listOfBigrams = []\n",
        "   bigramCounts = {}\n",
        "   unigramCounts = {}\n",
        "   for i in range(len(data)-1):\n",
        "      if i < len(data) - 1 and data[i+1].islower():\n",
        "\n",
        "         listOfBigrams.append((data[i], data[i + 1]))\n",
        "\n",
        "         if (data[i], data[i+1]) in bigramCounts:\n",
        "            bigramCounts[(data[i], data[i + 1])] += 1\n",
        "         else:\n",
        "            bigramCounts[(data[i], data[i + 1])] = 1\n",
        "\n",
        "      if data[i] in unigramCounts:\n",
        "         unigramCounts[data[i]] += 1\n",
        "      else:\n",
        "         unigramCounts[data[i]] = 1\n",
        "   return listOfBigrams, unigramCounts, bigramCounts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNw3ju8Q7pdk"
      },
      "outputs": [],
      "source": [
        "def calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n",
        "    listOfProb = {}\n",
        "    for bigram in listOfBigrams:\n",
        "        word1 = bigram[0]\n",
        "        word2 = bigram[1]\n",
        "        listOfProb[bigram] = (bigramCounts.get(bigram))/(unigramCounts.get(word1))\n",
        "    return listOfProb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHpbVrLJ7rtU"
      },
      "outputs": [],
      "source": [
        "\n",
        "data = readData()\n",
        "listOfBigrams, unigramCounts, bigramCounts = createBigram(data)\n",
        "\n",
        "print(\"\\n All the possible Bigrams are \")\n",
        "print(listOfBigrams)\n",
        "\n",
        "print(\"\\n Bigrams along with their frequency \")\n",
        "print(bigramCounts)\n",
        "\n",
        "print(\"\\n Unigrams along with their frequency \")\n",
        "print(unigramCounts)\n",
        "\n",
        "bigramProb = calcBigramProb(listOfBigrams, unigramCounts, bigramCounts)\n",
        "\n",
        "print(\"\\n Bigrams along with their probability \")\n",
        "print(bigramProb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFtvNtU_0xD2"
      },
      "outputs": [],
      "source": [
        "# Generate Sentence\n",
        "# Set lenghth of the sentence to 10\n",
        "max_len=15\n",
        "# Start word\n",
        "current=\"hello\"\n",
        "s=current+\" \"\n",
        "for i in range(max_len):\n",
        "  list_bigrams={}\n",
        "  # print(\"current word \", current)\n",
        "  for key_tuple in bigramProb.keys():\n",
        "\n",
        "  # Use the current word to see what is the most frequent word to follow it\n",
        "    if current in key_tuple[0].split():\n",
        "      # Save list of all bigrams where the current word first appears\n",
        "      list_bigrams[bigramProb[key_tuple]]=key_tuple\n",
        "  # print(\"list_bigrams\",list_bigrams)\n",
        "  # get the highest probability bigram for the current word\n",
        "  max_=max(list_bigrams.keys())\n",
        "  max_bigram=list_bigrams[max_]\n",
        "  # set the current word with the highest prob word for the next iteration\n",
        "  current=max_bigram[1]\n",
        "  # append to the sentence\n",
        "  s+=max_bigram[1]+\" \"\n",
        "print(\"Generated Sentence \",s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVg6ub9RlC73"
      },
      "outputs": [],
      "source": [
        "\n",
        "inputList=\"eh el a5abar\"\n",
        "# inputList=correct(inputList)\n",
        "splt=inputList.lower().split()\n",
        "outputProb1 = 1\n",
        "bilist=[]\n",
        "bigrm=[]\n",
        "\n",
        "for i in range(len(splt) - 1):\n",
        "    if i < len(splt) - 1:\n",
        "        bilist.append((splt[i], splt[i + 1]))\n",
        "\n",
        "print(\"\\n The bigrams in given sentence are \")\n",
        "print(bilist)\n",
        "for i in range(len(bilist)):\n",
        "    if bilist[i] in bigramProb:\n",
        "      print(bilist[i],bigramProb[bilist[i]])\n",
        "      outputProb1 *= bigramProb[bilist[i]]\n",
        "    else:\n",
        "        outputProb1 *= 0.0\n",
        "print('\\n' + 'Probablility of sentence '+inputList+' = ' + str(outputProb1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra Excercise"
      ],
      "metadata": {
        "id": "Qg36rAKh6lSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this excercise you should use the bigram model to generate the correct candidates for the personalized spelling checker\n",
        "Hint: use the probability as your context to decide on the correct replacment"
      ],
      "metadata": {
        "id": "o3hftn0N7EUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your code here"
      ],
      "metadata": {
        "id": "-GJySJlS7Ex8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since term matching is not an easy task for TFIDF systems, try and find the correct synonyms/antonyms that would help you pick the correct query for the retrival model\n",
        "\n",
        "Hint: use NLTK WordNet"
      ],
      "metadata": {
        "id": "GnpYilB27fWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your code here"
      ],
      "metadata": {
        "id": "nvQcFYE98SBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mehtTR9J2kwz"
      },
      "source": [
        "# Additional Resources\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsUzLXk32ocT"
      },
      "source": [
        "[fuzzywuzzy](https://www.geeksforgeeks.org/fuzzywuzzy-python-library/)\n",
        "\n",
        "[String matching with fuzzywuzzy](https://towardsdatascience.com/string-matching-with-fuzzywuzzy-e982c61f8a84)\n",
        "\n",
        "[Levenshtein_distance](https://en.wikipedia.org/wiki/Levenshtein_distance)\n",
        "\n",
        "[DrQA](https://github.com/facebookresearch/DrQA)\n",
        "\"DrQA adopts bi-gram hashing and TF-IDF matching to search over Wikipedia, given a natural language question.\"\n",
        "\n",
        "\n",
        "[How to compute text similarity on a website with TF-IDF in Python](https://towardsdatascience.com/how-to-compute-text-similarity-on-a-website-with-tf-idf-in-python-680b3be06091)\n",
        "\n",
        "[How to scrape a blog and collect its articles in Python](https://medium.com/mlearning-ai/how-to-scrape-a-blog-and-collect-its-articles-in-python-80895d8def66)\n"
      ]
    }
  ]
}